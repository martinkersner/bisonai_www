---
layout: post
title:  "Convolutional network without multiplication operation"
date:   2019-02-06 00:0:00 +0000
disqus_identifier: 2018-02-06
author: martin
comments: true
abstract: In september 2018 Google Research team released a paper with the title “No Multiplication? No floating point? No problem? Training networks for efficient inference” which will dub an NMNF from now on. The main building block in convolutional neural networks is convolutional layer and most of the time during inference (TODO link to vocabulary) is spent in those layers (TODO link to proof). NMNF paper targets devices like hearing aids, earbuds or wearables. Such devices are highly resource constrained, in term of memory, power and computation, and therefore benefit from a specialized implementation of convolutional layer introduced in paper. Inference-time floating point operations are expensive (TODO link to table), and NMNF approach avoids them fully within convolutional layers (TODO link to measuring speed for each layer). Their approach also leads to small model size (TODO how much link to lower part of blog).
---

In September 2018, Google Research team released paper with title **“No Multiplication? No floating point? No problem? Training networks for efficient inference”** <a href="#nmnf_ref">[1]</a>.
We will refer to it as NMNF.
The main building blocks of convolutional neural networks are convolutional layers and great majority of [inference]({{ "/glossary#inference" | prepend: site.url }}) time is spent in them <a href="#lmgemm_ref">[2]</a>.
TODO inference time breakdown
NMNF paper targets devices like hearing aids, earbuds or wearables.
Such devices are highly resource constrained, in terms of memory, power and computation, and therefore benefit from a specialized implementation of convolutional layer introduced in the paper.
Inference-time floating point operations are not only energy hungry compared to integer operations (see table 1) but also <a href="http://nicolas.limare.net/pro/notes/2014/12/12_arit_speed/#index3h2" target="_blank">computationaly demanding</a>.
NMNF approach avoids floating point operations entirely (TODO link to measuring speed for each layer) and as a side effect <a href="#memory-savings">model size is reduced</a>.

| integer  | 500 MHz | 1,200 MHz | floating point | 500 MHz | 1,200 MHz |
|----------|---------|----------|----------------|---------|-----------|
| **add**  | 63 pJ   | 105 pJ   | **fadd**       | 157 pJ  | 258 pJ    |
| **sub**  | 64 pJ   | 105 pJ   | **fsub**       | 158 pJ  | 259 pJ    |
| **mul**  | 116 pJ  | 189 pJ   | **fmul**       | 161 pJ  | 265 pJ    |
| **div**  | 178 pJ  | 286 pJ   | **fdiv**       | 566 pJ  | 903 pJ    |
{:.table}
*Table 1:
Integer and floating point arithmetic instructions with RAW dependencies measured on Cortex-A7 with frequencies 500 and 1,200 MHz. <a href="#compener_ref">[3]</a>*


[Mobile Deep Learning]({{ "/services" | prepend: site.url }}) is a field of Machine Learning that deploys optimized Deep Learning models on mobile devices (mobile phones, IoT, edge devices and others).
In this blog post we describe the main points of the NMNF approach and test if we can exploit proposed solution in mobile phones (TODO: link to implementation explanation).
Many Mobile Deep Learning applications require low latency so we will judge (TODO better wording) NMNF solution according to inference time.
Who would like to wait until the model predicts what objects are in image?
Users of such application certainly not.

## Training networks for efficient inference
The main idea behind NMNF is to precompute all possible products of input feature map with convolutional weights and store them in <a href="#lookup-table">lookup table (LUT)</a>.
Later, during inference, instead of performing convolution using multiply operations, lookup table is searched to obtain desirable multiplication results.


In the next sections, we assume that inside NMNF networks convolutional and activation layers periodically alternate (figure 1) and no other layers are utilized unless stated otherwise.

<center>
<img src="{{ "/img/blog/nmnf/act_conv_act_conv.png" | prepend: site.url }}" width="35%">
</center>

*Figure 1: In NMNF network, activation layer always follows convolutional layer.*


To limit the number of items in lookup table, both input feature maps and weights are quantized.
Following sections explain how to <a href="#activation-quantization">quantize input feature maps</a> and <a href="#weight-quantization">convolutional weights and biases</a>.


### Activation quantization
NMNF authors propose a quantization method that can be applied to any activation function.
Below, you can find PyTorch implementation of <a href="#act_quant_code">quantized $tanh$ function</a>.
The fidelity to original activation function is governed by number of activation levels, denoted as $|A|$.
The more activation levels we use the closer we are to the original non-quantized activation function at the expense of larger lookup table.
The way how lookup table is affected by the number of activation levels will be discussed later. (TODO link)

<a id="act_quant_code"></a>
```python
class tanhD(torch.autograd.Function):
  gamma_min = -1.0
  gamma_max = 1.0

  @staticmethod
  def forward(ctx, input: torch.autograd.Variable, levels: int):
    y = torch.nn.tanh(input)
    ctx.y = y

    step = (ctx.gamma_max - ctx.gamma_min)/(levels - 1)
    quant_y = torch.floor((y - ctx.gamma_min)/step + 0.5) * step + ctx.gamma_min
    return quant_y

  @staticmethod
  def backward(ctx, quant_y: torch.autograd.Variable):
    grad_input = 1.0 - ctx.y**2
    grad_levels = None
    return grad_input, grad_levels
```

*Code 1: PyTorch implementation of quantized $tanh$ function.*

Notice that plateaus are not equally sized in figure 2.
The shorter plateaus correspond to larger rate of change in activation function.

![]({{ "/img/blog/nmnf/qtanh_levels.png" | prepend: site.url }})

*Figure 2: Visualization of quantized $tanh$ function with different quantization levels.*

Quantization of activation function is performed only in forward pass.
In backward pass, quantization is ignored because quantized activation function is piece-wise constant function and have zero or undefined gradients.
Hence, gradients are computed from the original non-quantized activation function.


### Weight quantization
Weight quantization reduces the number of allowed weights (denoted as $|W|$) and keeps the size of lookup table invariant.
Unlike activation quantization which is performed at every training step, weight quantization is applied periodically at predefined intervals (e.g. 1,000 steps) on all the weights in the network (including biases).
Weights are trained as in regular convolutional network and only after weight quantization step, there are $|W|$ unique weights in the network.

Shumeet Baluja et al. suggest two ways of weight quantization: K-Means clustering and model-based quantization approach.

<a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" target="_blank">K-Means clustering</a> respects the underlying distribution of the weights, but with large number of weights (AlexNet has 60 million parameters <a href="#alexnet_ref">[4]</a>) the clustering process is slow.
An easy way out of this problem is to subsample weights for faster training, however it does not guarantees optimal clustering result.
Another solution can be to employ Mini Batch K-Means <a href="#batchkmeans_ref">[5]</a> which allows for faster and more fine-grained settings of clustering technique.

The second approach builds upon the knowledge that fully-trained weight distributions often resemble Laplacian or Gaussian distributions <a href="#nmnf_ref">[1]</a>.
If we approximate all weights with one distribution we can trace back the loss in accuracy to the overall $L_{1}$ or $L_{2}$ error.
$L_{1}$ Laplacian-based clustering model can be defined in closed form using extreme values for odd number of clusters, $N$.
Cluster centers $C_{i}$ lie at $a \pm bL_{i}$, where $a$ denotes the mean value of network weights, $b$ is scaling factor and $L_{i}$ is relative distance between the mean of weight distribution and its corresponding cluster centers.

$$
\begin{align}
L_{0} & = 0 \\
L_{i} & = L_{i-1} + \Delta_{i} \\
\Delta_{i} & = -ln(1 - 2 exp(L_{i-1})/N)
\end{align}
$$

*Equation 1: Computation of $L_{i}$, relative distance between the mean of weight distribution and its corresponding cluster centers and $\Delta_{i}$, the update for the next $L_{i}$ distance.*

Scaling factor $b$ is based on the cluster occupancy curve for given distribution.
The less samples are assigned to particular cluster, the larger scaling factor $b$ is.
At the beginning of training, the weights does not follow Laplacian distribution and therefore introduced model-based clustering has to be corrected for it.
Maximum quantization level $a \pm bL_{N/2}$ is set to be close to the maximum observed weight.
More information about scaling factor modifications during training you can find in NMNF paper.

## Lookup table
In the previous two sections, we explained how to quantize activation function and weights to $|A|$ activation levels and $|W|$ unique weights respectively.
Lookup table (figure 3) has $|A| + 1$ (for bias) rows and $|W|$ columns.
The entries in table are products of input activations and weights, $LUT_{i,j} = a_{i} * w_{j}$.
Note that for now, we keep floating point values in table.
Later, we will describe how to <a href="#remaining-inefficiencies">completely remove floating point operations from convolutional layer</a>.
Various experiments on AlexNet revealed that the best number of activation levels and unique weights is 32 and 1,000 respectively <a href="#nmnf_ref">[1]</a>.
With those hyperparameters, the table would contain 33,000 values (including additional 1,000 values for biases).
<a href="#memory-savings">Memory savings</a> achieved by NMNF approach are discussed later.

<center>
<img src="{{ "/img/blog/nmnf/lookup_table.png" | prepend: site.url }}" width="45%">
</center>

*Figure 3: Visualization of lookup table with precomputed $a_i * w_j$ products.
Notice that the last row of table is used for storing biases.*

The lookup table is used within convolutional layer and its values are accessed through input activations and weights (and biases) that are row and column indexes of lookup table respectively.

<center>
<img src="{{ "/img/blog/nmnf/quant_conv_diagram.png" | prepend: site.url }}" width="50%">
</center>

*Figure 4: Visualization of data flow in NMNF convolutional block.
Convolutional layers takes as input quantized activations (row indexes of lookup table), its weights and biases are stored as column indexes pointing to lookup table and the outputs of activation function are row indexes of lookup table.*

With lookup table, convolution and activation operation defined as above, the inference in convolutional layer (including activation function) for a single output value can be accomplished in following four steps:
1. Gather $LUT_{i,j}$ values in lookup table that correspond to $a_{i} * w_{j}$ instead of multiplying those values.
2. Sum up those values, $Y = \sum{LUT_{i,j}}$
3. Add bias, $Y_{bias} = Y + LUT_{\|A\|,j}$
3. Find the appropriate quantization value for $Y_{bias}$. The level of quantized value corresponds to the row index $i$ in lookup table.


Naive implementation of the last step, if we want to refrain from using floating point activation function, is to search boundaries of precomputed quantized activation function.
The downside of this approach is slow execution that increases with the number of levels.

In this section we explained how to remove floating point multiplication operation in convolutional layer and also how to implement non-linear activation function computation.
As might be expected, the next step is to completely avoid floating point values in convolutional layer and speedup activation function computation.


### Remaining inefficiencies
With current setup, even though we store all convolutional weights and biases as integer values (column indexes), we still need to accumulate values from lookup table that are stored in floating point representation.
By multiplying every entry of lookup table with large scale factor $S$ (different from scale factor described in Activation quantization TODO link section) we obtain fixed point representation of $a_i * w_j$ product.
Recommended scale factor is $2^s$ where $s$ should be selected empirically.
It is not necessary to use large scale factor $s$ but we should make sure that values we sum up inside convolutional layer can fit into accumulator data type without overflowing.
In order to convert values back to its unscaled form we perform right shift by $s$ bits.
Note that right bit shift is lossy operation where we remove $s$ least significant bits.
Furthermore, if we divide all values in lookup table by sampling interval $\Delta x$ that is equal to the width of every bin of quantized activation function, bitwise right shift will yield the index of bin to which it belongs to.
To summarize, every precomputed value in table needs to be multiplied by $\frac{2^s}{\Delta x}$ term.

Below, you can find a code snippet with implementation of convolutional operation and quantized ReLU6 using only integer summation and bit shift.

```python
def gen_linspace(
    min_input: float=-7.0,
    max_input: float=7.0,
    max_input_values: int=10_000,
) -> np.array:
  return np.linspace(min_input, max_input, max_input_values)

def reluD(input: List[float], levels: int) -> np.array:
  """ Quantize ReLU6 activation function to given number of `levels`.
  """
  gamma_min = 0.0
  gamma_max = 6.0

  left_boundary = np.ones(len(input)) * gamma_min
  right_boundary = np.ones(len(input)) * gamma_max

  relu = np.minimum(np.maximum(input, left_boundary), right_boundary)
  step = (gamma_max - gamma_min)/(levels -1)
  return np.floor(relu/step) * step + gamma_min

def reluD_bin_index(
    input: float,
    levels: int,
) -> int:
  """Search boundaries of quantized ReLU activation function with given
  number of `levels` and find index of bin to which `input` falls into.
  """
  x = gen_linspace().tolist()
  activations = reluD(x, levels)

  if input <= np.min(activations):
    return 0

  if input >= np.max(activations):
    return levels-1

  unique_activations = np.unique(activations)
  boundaries = activations[np.where(activations[1:] - activations[:-1])]

  for idx, (left, right) in enumerate(zip(boundaries, boundaries[1:])):
    if input >= left and input < right:
      return idx


# General settings
W = 1_000  # number of weights
A = 32  # number of activation levels
bit_precision = 8
scale_factor = 2**bit_precision
int_dt = np.int16

# Generate quantized ReLU6 activation values
activations_input = gen_linspace()
activations = reluD(activations_input.tolist(), A)
unique_activations = np.unique(activations).tolist()
assert len(unique_activations) == A

# Derive delta x - ReLU6 has equally sized plateaus
activation_boundaries = activations_input[np.where(activations[:-1] - activations[1:])]
delta_x = np.abs(activation_boundaries[0] - activation_boundaries[1])

# Sample random weights from Laplacian distribution
unique_weights = np.random.laplace(loc=0.0, scale=0.25, size=W).tolist()
assert len(unique_weights) == W

# Build lookup table
LUT = np.vstack([
    np.array(unique_activations).reshape([-1,1]) * np.array(unique_weights),
    np.array(unique_weights).reshape([1,-1])
])

# Build scaled lookup table
LUT_scaled = np.round(LUT * scale_factor / delta_x).astype(int_dt)

# Imitate convolution operation with kernel of size 3x3 and 1 input channel
# at one fixed location.
kernel_size = 3 * 3 * 1

# Generate random row (input activation) and column (weights and biases) indexes
# for lookup table.
row_indexes = np.random.randint(0, len(unique_acts), kernel_size)
column_indexes = np.random.randint(0, len(unique_weights), kernel_size)
bias_column_indexes = np.random.randint(0, len(unique_acts), 1)

# Sum up floating point values
fp_sum = LUT[row_indexes, column_indexes].sum() \
       + LUT[len(unique_acts), bias_column_indexes]

# Sum up values in fixed point representation
int_sum = LUT_scaled[row_indexes, column_indexes].sum() \
        + LUT_scaled[len(unique_acts), bias_column_indexes]

# Scan boundaries of quantized ReLU6 activation function to get index of bin
fp_act_index = reluD_bin_index(fp_sum, levels)

# Perform right bit shift to obtain index of bin to quantized activation function
int_act_index = np.maximum(np.minimum(int_sum.tolist()[0] >> bit_precision, levels-1), 0)

print("Floating point sum:", fp_sum)
print("Bin index of activation function obtained from floating point sum:", fp_act_index)
print("Integer sum:", int_sum)
print("Bin index of activation function obtained from integer sum:", int_act_index)
```

```
Floating point sum: [4.04475552]
Bin index of activation function obtained from floating point sum: 20
Integer sum: [5359]
Bin index of activation function obtained from integer sum: 20
```
*Code 1: TODO make description.*

One of the shortcomings of this approach is that plateaus are required to be equally sized, and it does not hold true for every activation function.
For example, $tanhd$ function has variable sized of plateaus (TODO link to image).
To combat this problem we can search for $\Delta x$ that would correspond to the greatest common divisor (GCD) of all plateau sizes.
Since there is no guarantee that such $\Delta x$ exists we might have to slightly shift boundaries of plateaus in order to fulfill GCD condition.

Right bit shift operation can still be used as a replacement of quantized activation function, however one extra step is needed.
The real indexes of quantized activation bins are stored in one-dimensional array (see figure 5) and accessed using the value obtained from the bit shift operation.
You can see that some indexes are repeated.
This allows to encode arbitrarily sized bins of quantized activation function.

<center>
<img src="{{ "/img/blog/nmnf/activation_array_with_delta_x.png" | prepend: site.url }}" width="60%">
</center>

*Figure 5: Array with indexes to quantized activation function.*


## Memory savings?
Regular neural network models take significant chunk of memory.
It is caused by the large number of weights and their floating point representation (32 bits per weight).
However, in case of NMNF, we need to store only column indexes to the fixed size lookup table.
For example, if we want to encode indexes to lookup table with 1,000 unique weights, we can represent them with only 10 bits ($2^{9} < 1,000 < 2^{10}$) per weight, which can save up to 68.75 % memory compared to the floating point model.
The paper does not mention it, but one of the current trends is to quantize weights to 8 bits <a href="#quantwhite_ref">[6]</a>.
Using 8 bit encoded weights yield **75 % memory savings with respect to floating point representation** and 19.9 % memory reduction compared to NMNF representation.

Further, the authors claim that applying entropy coding to weights indexes can decrease the index size from 10 to 7 bits.
We decided to put such claim to the test (see code 2).
First, we sampled 60 million weights (same as number of weights in AlexNet) from Laplacian distribution (see figure 5a) and clustered them with Mini Batch K-Means algorithm to 1,000 bins.
From figure 5b you can see that counts of weights in bins follow the Laplacian distribution as well.

<center>
<img src="{{ "/img/blog/nmnf/weight_distribution.png" | prepend: site.url }}">
</center>

*Figure 5: (a) Histogram of weights sampled from Laplacian distribution.
(b) After the weight clustering step, the bin weight counts keep the same data distribution as the original weights.*


Finally, we computed discrete probability distribution of clustered weight indices and information entropy.
According to <a href="https://en.wikipedia.org/wiki/Shannon's_source_coding_theorem" target="_blank">Shannon's source coding theorem</a> it is impossible to compress data such that the average number of bits per symbol would be less than information entropy of the data.
Our calculated information entropy was 9.756 bits which signals that weight indexes cannot be encoded with less number of bits.
Different weight indexes and clusters will yield different information entropy, but it is unlikely that 7 bits would be sufficient to encode them.

```python
import numpy as np
import scipy as sc
from sklearn.cluster import MiniBatchKMeans

def information_entropy(data: np.array, base: int=2) -> float:
  """Calculate the entropy of a given data stream.
  """
  unique_elements, counts_elements = np.unique(data, return_counts=True)
  p_data = counts_elements / data.size
  return sc.stats.entropy(p_data, base=base)

num_weights = 60_000_000  # number of weights in AlexNet
num_unique_weigths = 1_000  # number of columns in lookup table

# Sample random weights
W = np.random.laplace(size=(num_weights, 1))

# Cluster weights
kmeans = MiniBatchKMeans(
    n_clusters=num_unique_weigths,
    init="k-means++",
    max_iter=100,
    batch_size=100,
    verbose=0,
    compute_labels=True,
    random_state=None,
    tol=0.0,
    max_no_improvement=10,
    init_size=3*num_unique_weigths,
    n_init=3,
    reassignment_ratio=0.01,
).fit(W)

# Assign weights to clusters
W_clustered = kmeans.predict(W)

# Compute information entropy
H = information_entropy(W_clustered, base=2)

print("Information entropy:", H)
```

```
Information entropy: 9.756370565749936
```
*Code 2: Information entropy for 1,000 unique weights out of 60 million randomly sampled weights is 9.756&nbsp;bits.
This entropy can change based on the weight counts in every bin.*

## No multiplication, no floating point in AlexNet
In paper, NMNF approach was evaluated on AlexNet network.
The modified network achived comparable results with floating point model utilizing ReLU6 activation function.
The experiments have also shown that with quantized inputs the performance degradation is small.

| AlexNet                      | Recall@1 |  Recall@5 |
|------------------------------|----------|-----------|
| Floating point               | 56.4     | 79.8      |
| NMNF floating point inputs   | 57.1     | 79.8      |
| NMNF quantized inputs        | 56.9     | 79.4      |
{:.table}
*Table 2: Comparison of floating point and NMNF AlexNet model.
NMNF model employed 1,000 unique weights and 32 quantized levels of ReLU6 activation function.*

AlexNet contains one extra layer (dropout layer can be ignored during inference), **max pooling**, that has not been mentioned yet.
Max pooling layers come after activation layers (figure 6).
Fortunately, the order of quantized output values (row indexes of lookup table) correspond to the order of their real values, thus max pooling in NMNF network can be performed without any modifications.
On the other hand, if we had **average pooling layer** in our network we would have to convert indexes to their real values in order to compute average.
We could also, similarly to our previously defined <a href="#lookup-table">lookup table</a>, precompute all possible averages and then match them with pooling windows.

<center>
<img src="{{ "/img/blog/nmnf/alexnet.png" | prepend: site.url }}" width="90%">
</center>

*Figure 6: Visualization of AlexNet network.*


## Speed comparison
In the last part of our NMNF review, we will integrate convolution operation without multiplication into <a href="https://github.com/tencent/ncnn" taget="_blank">high-performance neural network inference computing framework called ncnn</a> and discuss its consequences on inference time.
We assume that NMNF network would be deployed on mobile devices with ARM based processors since they cover about <a href="https://en.wikipedia.org/wiki/ARM_architecture#Market_share">95&nbsp;% of market</a>.
The enabling technology behind fast execution of deep learning models on devices with ARM processors is <a href="https://developer.arm.com/technologies/neon">ARM Neon</a>, an advanced SIMD (single instruction multiple data) architecture extension for the Arm Cortex-A series and Cortex-R52 processors.
Parallelism, highly optimized math functions and support for both float and 8-bit integer operations are perfect fit for current deep learning models.
For example, convolutional layers are composed of multiply & accumulate operations and this exact combination of operations can be executed using single instruction <a href="http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0489i/CIHBGIGD.html">VMLA</a> (Vector MuLtiply Accumulate).
If we use both 8-bit weights and 8-bit activations, we can perform 16 multiply & accumulate operations in parallel!
You can confirm that every <a href="https://github.com/Tencent/ncnn/search?q=VMLA&unscoped_q=VMLA" target="_blank">major</a> <a href="https://github.com/tensorflow/tensorflow/search?q=VMLA&unscoped_q=VMLA" target="_blank">deep</a> <a href="https://github.com/pytorch/QNNPACK/search?q=VMLA&unscoped_q=VMLA" target="_blank">learning</a> inference engine utilizes it.

```c++
int8x16_t   vmlaq_s8(int8x16_t a, int8x16_t b, int8x16_t c);    // VMLA.I8 q0,q0,q0
```
*Code 3: `vmlaq_s8` ARM Neon instruction can perform 16 multiplications of 8-bit integer and 16 addition of 8-bit integers in parallel using one instruction.*

First, we <a href="https://github.com/Tencent/ncnn/blob/master/benchmark/benchncnn.cpp" target="_blank">measured inference time</a> of Alexnet floating point version using <a href="https://en.wikipedia.org/wiki/ARM_big.LITTLE" target="_blank">single big core</a> on <a href="https://en.wikipedia.org/wiki/Samsung_Galaxy_Note_3" target="_blank">Galaxy Note 3</a> (see Figure 7) and <a href="https://en.wikipedia.org/wiki/Samsung_Galaxy_S8">Galaxy S8</a>.
Before the actual measurements were taken, 10 forward pass warm-up runs were executed.
Following 10 runs were averaged to obtain the final layer-wise speed measurements.
You can notice that the slowest layer is the first convolutional layer and that convolutional layers takes the most of the time overall (91.4&nbsp;% for Galaxy Note 3 an 95.7&nbsp;% for Galaxy S8).
The reason that the first convolutional layer in AlexNet takes up so much time is its <a href="https://github.com/Tencent/ncnn/blob/5e07749a4ac1dd77ae53cd5b5fd700c3465816ef/src/layer/convolution.cpp#L361-L398" target="_blank">unoptimized implementation for kernels with uncommon sizes</a>.
For further measurements, we are not going to consider this layer (*conv1*) in our evaluation.

<center>
<iframe width="600" height="371" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vT16ALXUf5iKM114kdbjJKRydiB_vJHg5FWIRzEt7c98JTpYjhWiAi_01qwXKccQFbc9lCYUsOxQ12d/pubchart?oid=2082912511&amp;format=image"></iframe>
</center>
*Figure 7: Layer-wise time breakdown of float Alexnet model run on Galaxy Note 3.*

Next, we added NMNF convolution layers (3x3s1 and 5x5s1) supporting 32 activation levels and 1,000 unique weights. (TODO link to implementation)
To faciliate correct execution of NMNF convolutional layers we swapped position of convolutional layers and its succeeding activation layers.
This modification ensures that there is always limited number of unique activation levels coming to NMNF convolutional layers.
Since this swap would affect speed measurements of activation layers, we measured speed of only convolutional layers.

```c++
static void conv3x3s1_nmnf(
    const Mat& bottom_blob,
    Mat& top_blob,
    const Mat& _kernel,
    const Mat& _bias,
    const int lookup_table[NMNF_NUM_ACTIVATIONS][NMNF_NUM_WEIGHTS],
    const Option& opt
    )
{
    int w = bottom_blob.w;
    int h = bottom_blob.h;
    int inch = bottom_blob.c;

    int outw = top_blob.w;
    int outh = top_blob.h;
    int outch = top_blob.c;

    const int* kernel = _kernel;  // column index
    const int* bias = _bias; // column index to the last row of lookup table

    for (int p=0; p<outch; p++)
    {
        Mat out0 = top_blob.channel(p);

        const int bias0 = lookup_table[NMNF_NUM_ACTIVATIONS-1][bias[p]];
        out0.fill(bias0);

        const int* k = kernel + p*inch*9;
        const int* k0 = k;
        const int* k1 = k+3;
        const int* k2 = k+6;

        for (int q=0; q<inch; q++)
        {
            int* outptr0 = out0;
            int* outptr1 = outptr0 + outw;

            const int* img0 = bottom_blob.channel(q); // row index
            const int* r0 = img0;
            const int* r1 = img0 + w;
            const int* r2 = img0 + w*2;
            const int* r3 = img0 + w*3;

            int i = 0;

            for (; i+1 < outh; i+=2)
            {
                for (int ow=0; ow<outw; ow++)
                {
                    *outptr0 += lookup_table[r0[0]][k0[0]] + \
                                lookup_table[r0[1]][k0[1]] + \
                                lookup_table[r0[2]][k0[2]] + \
                                lookup_table[r1[0]][k1[0]] + \
                                lookup_table[r1[1]][k1[1]] + \
                                lookup_table[r1[2]][k1[2]] + \
                                lookup_table[r2[0]][k2[0]] + \
                                lookup_table[r2[1]][k2[1]] + \
                                lookup_table[r2[2]][k2[2]];

                    *outptr1 += lookup_table[r1[0]][k0[0]] + \
                                lookup_table[r1[1]][k0[1]] + \
                                lookup_table[r1[2]][k0[2]] + \
                                lookup_table[r2[0]][k1[0]] + \
                                lookup_table[r2[1]][k1[1]] + \
                                lookup_table[r2[2]][k1[2]] + \
                                lookup_table[r3[0]][k2[0]] + \
                                lookup_table[r3[1]][k2[1]] + \
                                lookup_table[r3[2]][k2[2]];

                    outptr0++;
                    outptr1++;

                    r0++;
                    r1++;
                    r2++;
                    r3++;
                }

                outptr0 += 2 + w;
                outptr1 += 2 + w;
            }

            // remaining
            for (; i < outh; i++)
            {
                for (int ow=0; ow<outw; ow++)
                {
                    *outptr0 += lookup_table[r0[0]][k0[0]] + \
                                lookup_table[r0[1]][k0[1]] + \
                                lookup_table[r0[2]][k0[2]] + \
                                lookup_table[r1[0]][k1[0]] + \
                                lookup_table[r1[1]][k1[1]] + \
                                lookup_table[r1[2]][k1[2]] + \
                                lookup_table[r2[0]][k2[0]] + \
                                lookup_table[r2[1]][k2[1]] + \
                                lookup_table[r2[2]][k2[2]];

                    outptr0++;

                    r0++;
                    r1++;
                    r2++;
                }

                outptr0 += 2;
            }

            k0 += 9;
            k1 += 9;
            k2 += 9;
        }
    }
}
```
*Code 4: Implementation of 3x3s1 NMNF convolutional layer.*

Figure 8 (TODO link) compares average speed of convolutional layers between our integrated NMNF and original floating point convolutions in ncnn.
NMNF convolutions are significantly slower than regular floating point convolutions for both tested devices.
Surprisingly, the speed of floating point convolutions is very similar, even though Galaxy Note 3 is 4 years older than Galaxy S8.

* it is slow because it cant be parallelized!
https://duckduckgo.com/?q=arm+neon+lookup+table&t=ffab&ia=web
https://stackoverflow.com/questions/26052553/how-a-more-than-256bytes-look-up-table-can-be-accessed-using-neon#26118040
https://stackoverflow.com/questions/22158186/arm-neon-how-to-implement-a-256bytes-look-up-table
multiplication is quite simple operation (lookup table would be good if the operation itself is more expensive thatn lookup search)
http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0472j/chr1360928368027.html
* DoReFa (Zhou et al. , 2016 ), which is closest to our performance, is 8 times slower than the baseline implementation, whereas we expect our implementation to be as fast as or faster than the baseline due to the relative speed of lookups versus multiplies.
* measure speed of float, integer and table on Alexnet?

<center>
<iframe width="600" height="371" seamless frameborder="0" scrolling="no" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vT16ALXUf5iKM114kdbjJKRydiB_vJHg5FWIRzEt7c98JTpYjhWiAi_01qwXKccQFbc9lCYUsOxQ12d/pubchart?oid=421998894&amp;format=image"></iframe>
</center>
*Figure 8: TODO BLABLA.*

TODO measured by executing from adb shell (could be incorrect! link to TF Lite!) but it does not matter since we compare relative speeds


Can NMNF be paralelized? No! Random lookup canno be parallelized. Many cache misses. SIMD has loop
TODO image layer wise speed breakdown
lookup table is HUGE!

discuss about inference time of convolutional network without multiplication operation


## Last bits and missing pieces
TODO: what about the first and last layer? is it float?
First layer accepts quantized or non quantized layers. Compare accuracy results, how much it drops.
They quantized both inputs and last layer convolutions.
TODO: can it be extended to other layers?
TODO: Show that convolutions takes most of the time in convolutional neural networks. Convolution consist of large of number of multiply add operations!
TODO show how to encode values, entropy encoding
TODO inference breakdown
TODO one additional column for the last layer!!


## Conclusion
TODO
* Good for special hardware which does nto support multiplication, for applications that do not require high rate response of for systems with very large lookup tables (preferably with parallel search)

## Ad
If you need our help let us know!

## References
<a id="nmnf_ref"></a>
[1] S. Baluja, D. Marwood, M. Covell, N. Johnston: No Multiplication? No floating point? No problem? Training networks for efficient inference, 2018, <a href="https://arxiv.org/abs/1809.09244" target="_blank">link</a>
<br/>
<a id="lmgemm_ref"></a>
[2] A. Anderson, A. Vasudevan, C. Keane, D. Gregg: Low-memory GEMM-based convolution algorithms for deep neural networks, 2017, <a href="https://arxiv.org/abs/1709.03395" target="_blank">link</a>
<br/>
<a id="compener_ref"></a>
[3] E. Vasilakis: An Instruction Level Energy Characterizationof ARM Processors, 2015, <a href="https://www.ics.forth.gr/carv/greenvm/files/tr450.pdf" target="_blank">link</a>
<br/>
<a id="alexnet_ref"></a>
[4] A. Krizhevsky, I. Sutskever, G. E. Hinton: ImageNet Classification with Deep Convolutional Neural Networks, 2012, <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank">link</a>
<br/>
<a id="batchkmeans_ref"></a>
[5] D. Sculley: Web-Scale K-Means Clustering, 2010, <a href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf" target="_blank">link</a>
<br/>
<a id="quantwhite_ref"></a>
[6] Raghuraman Krishnamoorthi: Quantizing deep convolutional networks for efficient inference: A&nbsp;whitepaper, 2018, <a href="https://arxiv.org/abs/1806.08342" target="_blank">link</a>


## TODO disqus
